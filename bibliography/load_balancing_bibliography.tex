\documentclass{article}
\usepackage[style=numeric]{biblatex}
\usepackage[margin=1.0in]{geometry}


\addbibresource{biblatex-examples.bib}
\addbibresource{load_balancing_bibliography.bib}

%% declare this to include abstrats in the printed bibliography
%%
\DeclareFieldFormat{abstract}{\par\small#1}
\renewbibmacro*{finentry}{\printfield{abstract}\finentry}


\begin{document}

\title{Load balancing bibliography}
\author{Alan Heirich and Karthik Murthy}

\maketitle

\section{Categories}

\subsection{introduction}

\begin{itemize}
\item
Unbalanced tree search benchmark used by lifeline mapper
\cite{Saraswat:2011:LGL:1941553.1941582}.
Is there a standard benchmark for AMR simulations?

\item
Does the LB algorithm depend on the application?  Does it depend on the programming model (Legion)?  Is it independent of both?

\end{itemize}

\subsection{Category axes}

\begin{itemize}
\item
local vs. global: does the entire computer system have to participate, or can a small subset balance locally?

\item
nearest neighbor vs. long range exchanges

\item 
static vs. dynamic

\item
expensive vs. cheap (related to static vs. dynamic)

\item
embarassingly parallel vs. interconnected: does mapping matter?

\item
continuous domain (mesh simulations) vs. discrete (tree search)

\end{itemize}


\subsection{diffusion}

\let\clearpage\relax
%\begin{center}
%\begin{tabular}{ |c|c| } 
% \hline
% \include{horton} & cell 1\\ 
% cell4 & \include{leiber} \\ 
% cell7 & cell8 \\ 
% \hline
%\end{tabular}
%\end{center}

\cite{Diamond:2017:DLB:3148226.3148236}\\

notes on \cite{Diamond:2017:DLB:3148226.3148236}
from Alan

finite element, finite volume unstructured adapting meshes

Diffusive partition improvement, application specified criteria

N-graph: hyper graph structure represents relations among elements

Diffusion is performed on the N-graph

A multigraph allows multiple edges between a pair of vertices

The N-graph does this for hyper graphs

Saves on memory versus just a graph

Imbalance = $T_max / T_mean$

Does not explain how to compute the transfer amount

Graph distance: migrate elements in order according to their distance from the cell center

Experiments : billion element mesh airplane tail structure

Argonne Mira blue gene Q

$128*2^{10}$ to $512*2^{10}$ elements cases

Showed reasonable improvement, 1.5 imbalance to 1.12

Did worse on larger problems



\cite{HORTON1993209}\\
\include{horton}


notes on \cite{HORTON1993209}
from Alan

fea unstructured adaptive mesh

No topology assumptions

Multilevel algorithm complexity is logarithmic in number of processors

Diffusion methods may require many iterations, see Boillat claims of $O(n^2)$ iterations on n processors

Claim: pairwise diffusion can result in large load imbalance (I don’t think this is true of Laplace iteration although low frequency disturbances subside slowly)
See Cybenko claim of $log_2(n)$ steps iteration, uses hypercube topology

The algorithm here achieves $O(log_2 n)$ but does not depend on topology

Local communication costs less than nonlocal : hypercubes (this will always be true, just how much)

Load balancer should respect existing adjacency relationships of the domain

topology of the mesh may not match topology of computers

Basic diffusion method pairwise exchange of $0.5*(l_i - l_j)$ units of work

Multilevel algorithm aims to eliminate large scale imbalances

At each level divide processors into two sets and balance them as with two individuals

No explanation of how to choose these sets

Proof that it takes $log_2(n)$ steps — duh

requires entire system rebalance at once, not a local method

Claims standard diffusion techniques are bad because they don’t guarantee number of iterations




\cite{Deng:2010:HDB:1889863.1889910}\\

notes on
\cite{Deng:2010:HDB:1889863.1889910}
from Alan

Efficient cell selection scheme

Local and global diffusion schemes, global performs best

Global means knows all servers, local means only knows nearest neighbors

Note: reference ou and ranka 1997 solve lb problem as linear programming

Distributed virtual environments prefer fast solution over optimal solution

Experiment using simulated workloads, virtual environment users moving through the environment, environment is partitioned into regions, one region per server


\cite{Lieber:2016:PDL:2966884.2966887}\\
\include{lieber}

notes on
\cite{Lieber:2016:PDL:2966884.2966887}
from Alan


Good survey paper, worth reading again

Compare diffusion to geometric and graph based methods on thousands of nodes

Space filling curves, recursive bisection, parMetis, hierarchical space filling curves

Concludes diffusion has advantages

	.	The second-order algo- rithm [15] extends OD such that the previous iteration’s transfer influences the current. The parameter β ∈ ( 0,2) controls the influence. Optimal values are derived in [9] 

	.	[9]  R. Elsa ̈sser, B. Monien, and R. Preis. Diffusion Schemes for Load Balancing on Heterogeneous Networks. Theory Comput. Sys., 35(3):305–320, 2002. 

	.	[15]  S. Muthukrishnan, B. Ghosh, and M. H. Schultz. First  and Second Order Diffusive Methods for Rapid, Coarse, Distributed Load Balancing. Theory Comput. Sys., 31:331–354, 1998. 

Survey based on improving diffusion

Original diffusion

Second order diffusion

Improved diffusion called cheby

Chemotaxis-inspired diffusion, additional round of exchange of capacity of the target node guides the diffusion locally

Dimension exchange - Xu and Lau

\cite{ROTARU2004481}\\

notes on
\cite{ROTARU2004481}
from Alan


Our contribution  can be summarized as follows: we give a direct explicit expression of the balancing flow generated by a generalized diffusion algorithm and we show that this flow has an interesting property, that it is a scaled projection of any other balancing flow in the same heterogeneous environment. We give estimations for the second largest eigenvalue of a generalized diffusion matrix and we estimate the complexity of the proposed algorithm.  We further show that this algorithm has a better convergence factor than the hydrodynamic algorithm [17,18]. Compared to other approaches, the one we consider here offers the advantage of not using parameters that are dependent upon the eigenvalues of the Laplacian of the communication graph. 

Solves load balancing and mapping

Since communication changes so frequently cannot afford to compute Laplacian eigenvalues

Analogy to markov chains

Connections between generalized diffusion matrices and Laplacian spectrum of the graph

Bounds on eigenvalues

Migration flow - expression for the flow

Good paper lots of analysis


Estimations were given for the maximum number of steps that such an iterative process may take to balance
 purpose, some general bounds were formulated for the second largest eigenvalue of a generalized diffusion matrix. These bounds were also used to show that there are generalized diffusion algorithms that theoretically converge faster than the hydrodynamic algorithm 


\cite{ParabolicLB}\\
\cite{CYBENKO1989279}\\
\cite{10.2307/2584287}\\

notes on
\cite{10.2307/2584287}
from Alan

uses successive over relaxation to find an optimal step size for convergence

I wonder: do these convergence issues really matter?  Is the simplest scheme good enough?





\cite{Boillat:1990:LBP:95324.95326}\\

notes on
\cite{Boillat:1990:LBP:95324.95326}
from Alan

Show polynomial time convergence to equilibrium

Remark 5. In the discrete case, i.e. working with individual processes, our problem is equivalent to the random walk problem in graphs[20] 
20. D. Aldous. ‘An inaoduction to covering problems for random walks on graphs’.J. Theoretical 
Probability, 2(1), 87-89 (1989).


\cite{XU199572}
\include{alanheirich}




\cite{SCHLOEGEL1997109}






\subsection{game theory}

\cite{GROSU20051022}




notes on
\cite{GROSU20051022}
from Alan


Static load balancing problem

Noncooperative game: processors work independently to arrive at equilibrium

Characterize Nash equilibrium and derive greedy algorithm to compute it

Assume Poisson arrivals, exponentially distributed task times

$\phi_i$ job generating rate at node I

$s_{ji}$ fraction of user j tasks to be sent to node i

$\mu_i$ processing rate at node i 

Load balancing strategy for user j is a vector of ${ s_{ji} }$

Minimize response time for user j

Remark: this is for multiple users (humans) submitting jobs to a distributed system (cluster).

Our case is one user (application) submitting tasks to an exascale system

“Best reply” for a user is a strategy that gives minimal response time for that user in light of other users strategies

Similar problem for one user treated in Tang and Chanson[35]
X. Tang, S.T. Chanson, Optimizing static job scheduling in a network
of heterogeneous computers, in: Proceedings of the International
Conference on Parallel Processing, August 2000, 373–382. Not very interesting.

Equation (8) defines the BEST\_REPLY solution

Execution time is O(n log n) due to the need to sort computers by workload

Otherwise it would be O(n)

This algorithm requires global knowledge of the workload of every computer at every agent and knowledge of all users strategies

Not scalable

Compared to two other lb schemes, one does a global optimization based on global knowledge, and an IOS scheme that gives good quality results

Experiments on 16 node cluster


\cite{doi:10.1142/S0219198902000574}

notes on
\cite{doi:10.1142/S0219198902000574}
from Alan

Establish uniqueness of Nash equilibria

No software experiment

Not relevant






\cite{7967109}

notes on
\cite{7967109}
from Alan

Entirely theoretical result, random movement of tasks with uniform weights



\cite{BELIKOVETSKY201616}

notes on
\cite{BELIKOVETSKY201616}
from alan

Load Rebalancing Games in Dynamic Systems with Migration Costs 
Initial assignment of jobs to identical parallel machines

Machines are added or subtracted

Extension parameter $\sigma$ is added to the cost of a job after it is moved (this is intended for a one time cost, not repeated)

Paper proves existence and calculation of Nash equilibrium and Strong Nash equilibrium

Under some assumptions any stable modified schedule approximates well an optimal schedule

“Each job incurs a cost which is equal to the total load on the machine it is assigned to” … ????

See game theoretic treatments of this problem 12, 2, 5, 8, survey in 17
	.	[2]  N. Andelman, M. Feldman, and Y. Mansour. Strong Price of Anarchy. In SODA, 2007 

	.	[5]  A. Czumaj and B. V ̈ocking. Tight bounds for worst-case equilibria. In ACM Transactions on Algo-  rithms, vol.3(1), 2007 

	.	[8]  A. Fiat, H. Kaplan, M. Levi, and S. Olonetsky. Strong Price of Anarchy for Machine Load Balancing. In ICALP, 2007. 

	.	[12] R.L. Graham. Bounds on Multiprocessing Timing Anomalies. SIAM J. Appl. Math., 17:263–269, 1969. 

	.	[17] B. V ̈ocking. In N. Nisan, T. Roughgarden, E. Tardos and V. Vazirani, eds., Algorithmic Game Theory. Chapter 20: Selfish Load Balancing. Cambridge University Press, 2007. 

$s_0$ assignment of $n$ jobs on $m_0$ machines
$m’$ machine are added or removed

Seek a Nash equilibrium $s$ where no machine can improve its situation by changing machines

``Assume that some preprocessing is done at the time a client is assigned to a server, before the download actually begins (e.g., locating the required file, format conversion, etc.). Clients might choose to switch to a mirror server. Such a change would require repeating the preprocessing work on the new server.   Another example of a system in which extension penalty occurs is of an RPC (Remote Proce- dure Call) service. In this service, a cloud of servers enables service to simultaneous users. When the system is upgraded, more virtual servers are added. Users might switch to the new servers and get a better service (with less congestion), however, some set-up time and configuration tuning is required for each new user.  Note that in all the above applications, the delay caused due to a migration is independent of the migrating job. 
’’

These sound like one-time costs to me hence my objection to this analysis, also the cost may depend on the job (amount of data movement, size of code, number of open files, etc)

Section 1.1 some issues with notation, use of $L_i(s)$ and $L_s(j)$ to mean different things?
“Price of anarchy” = ratio between max cost of a Nash equilibrium and the optimum schedule (load imbalance)
“Price of stability” = ratio between min cost of a Nash equilibrium and the optimum schedule

A set of players form a “coalition” if each job moves and strictly reduces its cost
Assignment is a “strong Nash equilibrium” if there exists no such coalition
Similar notions of strong price of anarchy, strong price of stability

``The simple greedy List-scheduling (LS) algorithm [11] provides a $(2 - 1/m)$approximation to the minimum makespan problem. A bit better approximation ratio of  $((4/3) - (1/3m))$ is guaranteed by the Longest Processing Time (LPT) algorithm [12]. A PTAS for the minimum makespan problem on identical machines is given in [13]. ‘’

``We show that any job scheduling game with added or removed machines possesses at least one Nash equilibrium schedule. Moreover, some optimal solution is also a Nash equilibrium, and thus, the price of stability is 1. We show that in general, the price of anarchy is unbounded when machines are either added or removed. ‘’

``We note that in a dynamic setting in which machines are added or removed and migrations 
are free of cost (i.e., when $\sigma$ = 0), then the results known for classic load balancing games apply.   In particular, the P oA assuming $\sigma$ = 0 is $2 - (2 / (m+1))$ for a game with m machines in the modified $m+1$  systems. The proofs are identical to the proofs for a fixed number of machines. Thus, the difference between our results and the results for the classic load balancing game are due to the migration penalty. ‘’

Sections on machine addition, machine removed, coalitions.  Each case proves existence of solutions.




\cite{BARAM2014241}


Reoptimization of the minimum total flow-time scheduling problem 

Guy Baram, Tami Tamir

Dynamic load balancing, accounts for migration cost
Optimal algorithm to find optimal solution with minimal migration cost (transition cost)
Minimal solution can be found by greedy algorithms, Shorted Processing Time(SPT) assign jobs in nondecreasing order by length
Example applications: manufacturing systems with jobs migrated along production lines; live migration of running VMs in a cloud data center; RPC servers

Other papers that minimize the migration cost:

	.	[10]  C. Clark, K. Fraser, S. Hand, J.G. Hansen, E. Jul, C. Limpach, I. Pratt, A. Warfield, Live migration of virtual machines, in: The 2nd Symposium on Networked Systems Design and Implementation (NSDI), 2005. 

	.	[19]  S. Hacking, B. Hudzia, Improving the live migration process of large enterprise  applications, in: The 3rd International Workshop on Virtualization Technolo-  gies in Distributed Computing (VTDC), 2009.  

J0 set of n0 jobs
M0 set of m0 identical machines
Pj is processing time for job j
S0 schedule of the initial instance
Changes include adding/deleting jobs from J0, machines from M0, and changing values of pj

A machine can process at most one job at a time
Price list theta(i,i’,j) is cost to migrate job j from machine I to machine I’
Job extension penalty delta(i,i’,j) is time extension increase in pj after moving job j from machine I to I’
Cj completion time of job j
Minimize sum Cj = “total flow time”

Two questions:
1. Reschedule to find minimum possible transition cost
2. Reschedule within a given budget B

Section 2, time t=0: optimal algorithm for rescheduling, complexity based on complete matching in bipartite graph with O(nm) vertices
Section 3, time t>0:
Section 4: unit migration costs and no job extension penalty.
Section 5: rebalancing with a budget
Section 6: results and discussion

Section 2 ——————————

	.	[7]  J.L. Bruno, E.G. Coffman, R. Sethi, Scheduling independent tasks to reduce mean finishing time, Commun. ACM 17 (1974) 382–387.

	.	[18]  W. Horn, Minimizing average flow-time with parallel machines, Oper. Res. 21  (1973) 846–847. 

Problem instance: nxm matrix of job times, pij is time for job j on machine I
Seems like a bug in the paper with indexing here, I is jobs and j is machines

Construct a bipartite graph, nodes V = J u U
J is the set of n jobs
U is the set of mn nodes

“Node qik is the kth from last position on machine I”
Edges completely connect nodes in J to nodes in U
Edge weights are w’(vj, qik) = k p’ij
“If job j is the kth from the last job to run on machine I, it contributes k times p’ij to the sum of completion times”

?? Why k times?

Optimal solution is found by min-weight matching in the bipartite graph.  (Obvious)

*** this algorithm is not distributed, requires global knowledge and centralized computation, ergo not scalable












\subsection{mapping}

\cite{doi:10.1137/0611030}


Heuristic algorithm: compute Fiedler vector, use it as an edge separator, then find the vertex separator by maximum matching in a subgraph

Require the two parts of the graph to be “roughly equal” in order to achieve load balance (not exactly equal)

Number of zero eigenvalue of laplacian matrix is equal to the number of connected components of the graph

Lots of definitions of Fiedler vector, laplacian matrix, spectral theory

4. Partition of grid graphs

Start with a “path graph” ?

Laplacian matrix is tridiagonal -> path graph is x 0 x diagonal pattern

5 spectral partitioning algorithm

Compute Fiedler vector

Use median value of vector elements to partition vector into left, right halves
This is an edge separator

We require a vertex separator
Simplest method is to choose the smaller of the two  endpoint sets (vertices divided by the edge separator)

But we want to compute the smallest separator
Solution is a vertex cover of the bipartite graph
Remove vertices a,b such that every edge is incident on one or both of these vertices

To find a minimum vertex cover use maximum matching algorithm

Worst case time complexity for algorithm to find one partitioning is O(sort ne)
Where e is number of edges, n is the length of the Fiedler vector



\cite{4227986}


Hyper graph partitioning for dynamic lb.  minimizes sum of communication costs plus migration cost.

Parallel multilevel repartitioning with Zoltan load-balancing toolkit

Results quality compressed favorably to ParMETIS

$t_total = alpha ( t_compute + t_communication ) + t_migration + t_repartitioning$

They ignore $t_compute$ and $t_repartuition$: for $t_compute$ they assume a balanced workload; for $t_repartition$ they assume this time is negligible

Net = hyper edge (edge with more than two vertices)

Vertices have weights wi 

Nets have costs cj (like edge weights)

Partition the hyper graph - each partition corresponds to one computer

Hypergrah H, Partition P

Wp = sum of vertex weights in partition Vp

P={Vi}. Partition consists of a set of parts 

Connectivity $\lambda_j$ of net j is the number of parts it connects

Multilevel partitioning - fine to coarse strategy, partition the coarse graph project back up

Their model combines communication cost with migration cost

Epoch = execution in time between two dynamic rebalance operations

Repartitioning hyper graph - augment the current epoch hyper graph Hj with additional vertices and nets to model data migration costs.  Now partition the resulting hyper graph using fixed vertices.

$H^j = (V^j, E^j)$ is the hyper graph that models epoch j of the computation

Rebalance following each epoch

Construction of the repartitioning hyper graph:
Add k new partition vertices ui, with zero weight, for each of k partition
For each vertex v in the old hyper graph, add a ”migration net” from v to new vertex ui if v was in partition I

Figure 1 is confusing read the text carefully

Vertex ui must be fixed to partition I

Now partition the new hypergraph

If the partition cuts a migration net then the cost of migration gets counted in the overall communication cost
This is the key idea that accounts for migration cost

Parallel multilevel hypergdaph partitioning with fixed vertices

Hg partition is NP hard, approximate using multilevel heuristics

Like multigrid,  coarsen the graph and partition the coarsest level

Propagate the partitions upward to the full graph


\cite{doi:10.1142/S0129054197000215}


\cite{Sbirlea:2014:BMS:2628071.2628090}


Concerned with scheduling when memory is limited.

“For many parallel applications, the memory requirements can be significantly larger than for their se- quential counterparts and, more importantly, their memory utilization depends critically on the schedule used when run- ning them. “

“Using the inspector/executor model, BMS tailors the set of allowable schedules to either guar- antee that the program can be executed within the given memory bound, or throw an error during the inspector phase without running the computation if no feasible schedule can be found. “

“Since solving BMS is NP-hard, we propose an approach in which we first use our heuristic algorithm, and if it fails we fall back on a more expensive optimal approach which is sped up by the best-effort result of the heuristic. “
“Unfortunately, parallel execution is known to increase mem- ory requirements compared to a serial baseline [8,11]. “	.	

[8] Robert D. Blumofe and Charles E. Leiserson. “Scheduling multithreaded computations by work stealing”. In: J. ACM (1999). 

[11] F. Warren Burton. “Guaranteeing Good Memory Bounds for Parallel Programs”. In: IEEE Trans. Softw. Eng. (1996) “

[16] 	.I. Dooley et al. “A study of memory-aware scheduling in message driven parallel programs”. In: HiPC. 2010  

This problem is a general case of the register sufficiency problem (NP hardness comes from this)

Inspector/executor: inspector builds a task graph that shows parent-child task relationships and reader-writer relationship for the data (similar to Legion).
Inspector identifies scheduling restrictions that lead to bounded-memory execution.  Enforce these restrictions in the executor stage when the app runs a load balancing work-stealing scheduler.

Contributions:


Heuristic algorithms for bounded memory scheduling (BMS) based on inspector/executor
Optimal algorithm based on ILP

Schedule memoization for heuristic algorithm

Concurrent Collections Programming Model (CnC)

Tasks (called steps)

Step tag - identify specific instance of a step (task)

items - dynamic single assignment variables

Item collections - identified by key, collect related items together like a struct

Step reads items by calling item\_collection.get(key), blocks waiting for data
They claim that building the task graph dynamically is too late for bounding memory consumption (?)

“Many analyses of task-parallel programs (such as data race detection) require understanding the task-parallel struc- ture of the computation, which is usually unknown at com- pile time. As a result, many of these analyses build the task graph dynamically, while the application is running. Unfor- tunately, this is too late for certain optimizations, such as bounding the memory consumption of the program. “

“Expansion functions” augment the CnC task graph to provide the necessary information

*******
“If the keys of items read and written and tags of steps spawned are only a function of the current step tag, then the application has independent control and data, which is needed to accurately model an application using BMS. “ *****
Sounds terrible!

“If the keys and tags depend on the values of items, we say that the application has coupled control and data. 
When faced with an application with coupled control and data, one possible solution is to include more of the com- putation itself in the graph expansion functions. In the extreme case, by including all the computation in the ex- pansion functions, we would be able to obtain an accurate dynamic task graph. Unfortunately, in the worst case, the computation would be performed twice, once for the expan- sion and once for the actual execution. However, our expe- rience is that many application contain independent control and data, thereby supporting the BMS approach. “

Assume all tasks have unit size, relax this in section 8

BMS heuristic algorithm is based on list scheduling

If no acceptable solution is found, run the optimal algorithm

5.1 successive relaxation of schedules
Sort depth-first rather than breadth-first in the task graph (duh)

5.2 color assignment
Determines concurrent chains of tasks

6. Optimal algorithm through ILP

8.1 supporting multiple item sizes
Assigns memory locations, deal with fragmentation that results from different sizes




\subsection{heterogeneous}

\cite{Flegar:2017:OLI:3149704.3149767}



Overcoming Load Imbalance for Irregular Sparse Matrices 
Flegar and anzt. $IA^3$ ’17 : Irregular Applications Architecture and Algorithms

GPU implementation of sparse matrix-vector product
Uses COO format, row-column index of each element
ELL format - store only nonzero, pad with zeros to give same vector length, for SIMD

This is not a load balancing or mapping algorithm.  This is a matrix multiply algorithm that tries to be load balanced.


\bigskip

\cite{8082085}

\bigskip
Dynamic Load Balancing on Multi-GPUs System for Big Data Processing 

This paper presents a novel dynamic load balancing model for heterogeneous multi-GPU systems based on the fuzzy neural network (FNN) framework. The devised model has been implemented and demonstrated in a case study for improving the computational performance of a two dimensional (2D) discrete wavelet transform (DWT). 

Chen et al. [7] proposed a task-based dynamic load balancing solution for multi-GPU systems that can achieve a near-linear speedup with the increase number of GPU nodes. Acosta et al. [8] had developed a dynamic load balancing functional library that aims to balancing the load on each node according to the corresponding system runtime. However, these pilot studies are based on the assumptions that all GPU nodes equipped in a multi- GPU platform have equal computational capacity. In addition, the task-based load balancing schedulers these approaches relied upon fall short to support applications with huge data throughputs but limited processing function(s) since there are very few “tasks” to schedule, e.g. DWT. 

	.	[7]  L.Chen,O.Villa,S.Krishnamoorthy,andG.R. Gao, “Dynamic Load Balancing on Single- and Multi-GPU Systems,” Ipdps, 2010.  

	.	[8]  A.Acosta,R.Corujo,V.Blanco,F.Almeida,H.P. C. Group, and E. T. S. De Ingenier, “Dynamic Load Balancing on Heterogeneous Multicore / MultiGPU Systems,” 2010.  

Their algorithm: divide work into equal size units; dynamically schedule those units according to evolution of dynamic workload; use fuzzy neural network to do this (?)

Fuzzy neural network predicts execution time for each GPU based on: flops rate; memory size; parallel scaling; occupancy rate of compute resources; occupancy rate of global memory.
Train neural network based on historical records.  (Main advantage seems to be that it allows GPUs to have different capacities).


CASE STUDY - discrete wavelet transform
Same amount of work per work-item.
Study used two different GPUs of different capacities.
Only experimented with two GPUs.

I do not think this paper is a good idea.


\cite{7993387}

A performance, power, and energy efficiency analysis of load balancing techniques for GPUs 

Regular and irregular workloads, hard to give good performance in both cases.
Dynamic multi-phase workload partition and work item-to-thread allocation.
Low complexity, good results.
Paper compares this to several other state of the art GPU lb algorithms.


NV Maxwell GTX 980, NV Jetson Kepler TK1 (low power embedded).
Work-units are grouped into work-items.
BFS graphs: work-units are graph vertices; work-items are neighbors of each work-item.
Prefix-sum-array holds offset to each work-item.


PRIOR ART:

Static mapping:


[4] P. Harish and P. J. Narayanan, “Accelerating large graph algorithms on the GPU using CUDA,” in Proceedings of the 14th International Conference on High Performance Computing, ser. HiPC’07, 2007, pp. 197–208. 
Simplest, one work item per thread, not load balanced

[5] S.Hong,S.K.Kim,T.Oguntebi,andK.Olukotun,“AcceleratingCUDA graph algorithms at maximum warp,” in Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming, ser. PPoPP ’11, 2011, pp. 267–276. 
Virtual warp (group of threads): workload assigned to threads of same group almost equal, therefore reduce branch divergence and improve memory coalescence.
Have to size the virtual warp correctly to get performance, this is a static parameter.
Good lb within a warp but not balanced across warps.

Semi-dynamic mapping:

[6] F. Busato and N. Bombieri, “BFS-4K: an efficient implementation of BFS for kepler GPU architectures,” IEEE Transactions on Parallel Distributed Systems, vol. 26, no. 7, pp. 1826–1838, 2015. 
Dynamic virtual warps: VW calculated at runtime, lb within a warp and across warps.

[7] D. Merrill, M. Garland, and A. Grimshaw, “Scalable GPU graph traversal,” in Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP ’12, 2012, pp. 117–128. 
Perfect balance among threads and warps, but expensive strip-mine first step, not worth it for regular workloads, good for irregular.

Dynamic mapping:

All require binary search across prefix-sum array

[8] A. Davidson, S. Baxter, M. Garland, and J. D. Owens, “Work-efficient parallel gpu methods for single-source shortest paths,” in Parallel and Distributed Processing Symposium, 2014 IEEE 28th International. IEEE, 2014, pp. 349–359. 
Load prefix-sum chunks to GPU shared memory, process on GPU, do not guarantee balance across blocks, nor memory coalescing.

[9] O. Green, R. McColl, and D. A. Bader, “Gpu merge path: a gpu merging algorithm,” in Proceedings of the 26th ACM international conference on Supercomputing. ACM, 2012, pp. 331–340. 
[10] “Modern gpu library.” [Online]. Available: http://nvlabs.github.io/ moderngpu/ 
9 and 10 propose similar methods, two phases: partition and expand.
First partition prefix-sum array into balanced chunks.
Second expand all threads load their chunks into shared memory, each thread binary searches the prefix-sum array to get the first assigned work unit.
Balanced across threads, warps and blocks at the cost of two binary searches.
Problem: memory accesses of the threads to work-units is badly scattered in memory.

[12] F. Busato and N. Bombieri, “A dynamic approach for workload partitioning on GPU archi- tectures,” IEEE Trans. on Parallel Distributed Systems, vol. preprint, no. 99, pp. 1–15, 2016. 
Multiphase: like two phase but at lower cost.

A. Multi-Phase Technique

Hybrid partitioning phase: each thread searches work-items switching between optimized binary search and interpolation search.
Iterative coalesced expansion phase: all threads load chunks into shared memory; each thread does optimized binary search to get the work-unit; three iterative sub-phases reorganize memory access for better coalescence.

1 writing on registers

2 shared memory flush, data reorganization

3 coalesced memory access

Steps 2 and 3 repeat until all are processed.  Results perform better due to improved memory coalescence.

Results (lots of data):
Their method how lowest execution time and lowest energy use out of 12 methods.  Next best was virtual warps.


\cite{Cederman:2008:DLB:1413957.1413967}


2008 graphics hardware
On dynamic load balancing on graphics processors

Task weight not known in advance, new tasks created dynamically during execution.
Compare 4 dlb methods, 1 is lock based.
Test problem is octree partition of particles.
Result: synchronization is very expensive, lock-free is better.

4 dlb methods:
Centralized blocking task queue (lock based),
Centralized non blocking task queue,
Centralized static task list,
Task stealing.

Thread level scheduling.
Gpu thread block: equal size, all on one processor, fast local memory, barrier.
Warp: 32 consecutive threads.
Very old processors: 9600GT 512MB 64 cores.

Task stealing performed best.


\cite{10.1007/978-981-10-6442-5_56}

A Load Balancing Strategy for Monte Carlo Method in PageRank Problem.
PageRank can be solved by Monte Carlo (known result).
Implement PageRank on GPU, deal with instruction divergence.
“Adopt the low-discrepancy sequences to simulate the random walks in PageRank computations”
“Each thread of a block to compute a random walk of each vertex with a same low-discrepancy sequence”

PageRank: list the relevant pages in order.
It’s a stationary vector of a random walk that simulates the process of surfing the internet.
Interpret as frequency that a random surfer browses the web page, similar to popularity.

Connectivity matrix P defines all hyperlinks.
P-ij = 1/k if page has k outgoing links and j is one of the links,
 = 1/n if page has no outgoing links,
Else = 0.
Surfer will choose next page from one of the links at random with p=c otherwise randomly from the web with p=(1-c).
So this is a Markov process with transition matrix

PP = c P + (1 - c) (1/n) E

Damping factor c = 0.85,
E is matrix of all ones.
The PageRank is the stationary distribution of the Markov chain, row vector pi such that
pi PP = pi , pi 1 = 1

Linear algebra methods compute pi using power method iteration, pi := pi PP, approximately linear in n.
Monte Carlo methods are faster and highly parallel.
This paper considers efficient Monte Carlo on GPU.

Optimization issues:

Instruction divergence

Load balancing between threads of a warp

Memory access conflicts between threads

Cache performance among threads

Local memory utilization

Instruction divergence in Monte Carlo occurs because random numbers are different so Markov chains diverge.
So they replace random sequences with “low discrepancy” sequences that are repeatable and identical, they reduce the variance of Monte Carlo Sampling [9-11]

9. Cervellera, C., Macciò, D.: Low-discrepancy points for deterministic assignment of hidden  weights in extreme learning machines. IEEE Trans. Neural Netw. Learn. Syst. 27(4), 891– 896 (2016) 

	10.	Gan, G., Valdez, E.A.: An empirical comparison of some experimental designs for the valuation of large variable annuity portfolios. Dependence Model. 4(1) (2016) 

	11.	Zapotecas-Martínez, S., Aguirre, H.E., Tanaka, K., et al.: On the low-discrepancy sequences and their use in MOEA/D for high-dimensional objective spaces. In: 2015 IEEE Congress on Evolutionary Computation (CEC), pp. 2835–2842. IEEE (2015)  
We propose a divergence avoidable strategy to allocate the threads of a block to compute a random walk of each vertex with identical low-discrepancy sequence. Hence, the threads of a block will simultaneously execute same instruction to compute next state of Markov chain or terminate. We can address the instruction divergence by our strategy. 

The static load balancing strategy pre-computes the length of each Markov chain to reduce the cost of absorbing state determination. The dynamic load balancing strategy dynamically determines the length of Markov chains to avoid the cost of loop condition determination. We will use experiments to test the efficiency of two strategies. 

Another optimization is that we load the low-discrepancy sequences into shared memory to speed up data fetch operations. 

The experiments indicate the bottleneck of our strategy is the memory access conflicts between threads. 
we store the adjacency matrix of a graph as the Compressed Sparse Row (CSR) format 
Since each random walk can be simulated independently, we can easily assign the computation of each walk for each thread. Furthermore, the all random walks begin from same node will be assigned to one block and all threads in a block simulate all random walks starting from one vertex in parallel. 
the time performance of a block will be critically influenced by the thread with longest random walks. 
So we try to use the determinacy of quasi random numbers to solve this problem.

Static load balancing

Wk, the expectation number of random walks whose length is k. we can call T kernels which have constant number of threads, Wk, each kernel computes k steps so that random walks in block have same length, in other words under same workload.   
all threads in a block have same workload, as a result, the warp execution efficiency will be improved. 
Their experiments show the dynamic lb made a minor improvement to the static lb case.

This is a good paper.





\cite{dlbgraphgpu}


Dynamic Load Balancing Strategies for Graph Applications on GPUs 

Existing work: node-based work-assignment to threads, gives poor load balance; edge-based requires lots of memory.
New work: three improvements to ameliorate these problems

Breadth-first search BFS;
Single source shortest past SSSP;
Minimum spanning tree MST;
“Betweenness centrality”;
Graph500 benchmark database.

Node-based assignment leads to load imbalance if the degree of the nodes is not uniform, works well for data in Compressed Sparse-Row (CSR) format.

Edge-based don’t have this problem but require reformatting the data to Coordinate List (COO).  Takes too much storage to handle large graphs on limited GPU memory.
Edge based requires distributivity in the operator which is not always possible,

Three contributions:
Workload decomposition - assign edges, but only for nodes on the active list
Node splitting - split a high degree node into multiple low degree nodes
Hierarchical processing - hierarchy of work lists

Evaluated on BFS and SSSP algorithms

Workload decomposition:

In this approach, the processing elements in the worklist continue to be the nodes, but the workload of the nodes, namely, the edges, are decomposed across threads using a block distribution. E number of graph edges are partitioned across T threads such that each thread receives a contiguous chunk of E/T edges for processing. Thus, a given thread processes a subset of edges corresponding to a subset of nodes and all the edges outgoing from a node may not be processed by the same thread. 

An advantage of workload decomposition is that it works with the CSR format and therefore, has a lower space com- plexity. 
A drawback of the workload decomposition is that it can lead to uncoalesced accesses since a node’s edges may get separated. 
In our experiments, we observe that the limitations of work- load decomposition affect its performance for large-diameter graphs (such as the road networks) but the method performs very well for scale-free graphs such as the social network 

Node splitting

node splitting preprocesses the graph to split each high-degree node into multiple low-degree child-nodes.
node-splitting approach has the advantage that it can work with the space-efficient CSR representation. 
A salient feature of our node splitting strategy is to automatically determine the threshold MDT for node splitting. Obvious methods based on a threshold or max-degree etc. do not work in general.   we use a histogram based method in which we use HistogramBinCount number of bins representing the ranges of out-degrees of the nodes in the original graph. 

An advantage of the node splitting approach is that it con- tinues to work with the space-efficient CSR format.
all the edges of a node are processed by the same thread, reducing bookkeeping and improving the scope for memory coalescing. The primary disadvantage of node splitting is that it results in extra atomic operations to update the child nodes whenever the parent node gets updated. A secondary disadvantage is the overhead of computing the histogram to find the MDT. 

node- splitting provides considerably better load-balancing. In ad- dition, it provides comparable performance for large diameter graphs (such as road networks); but it has a high overhead for power-law degree distribution graphs. 

Hierarchical processing

Hiearchical processing performs a time-decomposition of the workload. It achieves this by partitioning the main (super) worklist into several sub-worklists. If the sub-worklist is large, it can be further partitioned into sub-sub-worklists, and so on. This builds a hierarchy of worklists. The depth of this hierar- chy is tunable, and we utilize the histogram-based approach in node-splitting (Section III-B) for finding the maximum degree threshold (MDT) which determines when to split a worklist into sub-worklists. 

Compared BFS and SSSP to implementing from LonestarGPU which uses node-based distribution.


\subsection{large scale}

\cite{PEARCE2017}
\cite{BERLINSKA201814}
\cite{8017633}
\cite{DEVINE2005133}
\cite{javataskpool}
\cite{barat:tel-01672546}



\subsection{task based}

\cite{CPE:CPE1631}
\cite{Bhatti2017}
\cite{5599103}
\cite{Posner2018}
\cite{CCGrid2018}
\cite{8025281}
\cite{7307597}
\cite{Galvez:2017:ATM:3079079.3079104} % Charm++


\subsection{Work Stealing}
\cite{Yang2017}
\cite{Chen:2015:LWS:2775085.2766450}
\cite{Blumofe:1999:SMC:324133.324234}
\cite{Cilk}
\cite{Saraswat:2011:LGL:1941553.1941582}

\subsection{other}

\cite{Gao:2017:MPL:3110224.3110240}
\cite{CAMPOS20001213}
\cite{PINAR2004974}
\cite{7551381}
\cite{Menon:2013:DDL:2503210.2503284}
\cite{Liu:2017}
\cite{SEVERIUKHINA2017139}
\cite{7965131}


\subsection{reviews}

\cite{Teresco_2partitioning}

\printbibliography

\end{document}
